{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A100 GPU Optimized - Hidden Letters Classification\n",
    "## Optimizations Applied:\n",
    "- Mixed Precision Training (FP16)\n",
    "- XLA Compiler\n",
    "- Increased Batch Size (8 -> 64)\n",
    "- tf.data API with prefetch\n",
    "- cuDNN Auto-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup & GPU Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# GPU Memory Growth (prevent OOM)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU detected: {len(gpus)} device(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable XLA Compiler for faster computation\n",
    "tf.config.optimizer.set_jit(True)\n",
    "print(\"XLA JIT Compilation: Enabled\")\n",
    "\n",
    "# Enable Mixed Precision (FP16) - A100 Tensor Core Optimization\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(f\"Mixed Precision Policy: {policy.name}\")\n",
    "print(f\"Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"Variable dtype: {policy.variable_dtype}\")\n",
    "\n",
    "# cuDNN Auto-tune\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '0'\n",
    "os.environ['TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32'] = '1'\n",
    "print(\"cuDNN Auto-tune: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path configuration\n",
    "base_path = '/content/drive/Othercomputers/ë‚´ PC/ROKEY_2526/dacon_hidden_letters/'\n",
    "\n",
    "train = pd.read_csv(base_path + 'train.csv')\n",
    "test = pd.read_csv(base_path + 'test.csv')\n",
    "sub = pd.read_csv(base_path + 'submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Submission shape: {sub.shape}\")\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display label distribution\n",
    "print(\"Label Distribution:\")\n",
    "print(train['digit'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "train_features = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "test_features = test.drop(['id', 'letter'], axis=1).values\n",
    "train_labels = train['digit'].values\n",
    "\n",
    "# Reshape to image format (28x28x1)\n",
    "train_features = train_features.reshape(-1, 28, 28, 1)\n",
    "test_features = test_features.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Normalize to [0, 1] and convert to float32 (mixed precision will handle FP16)\n",
    "train_features = train_features.astype(np.float32) / 255.0\n",
    "test_features = test_features.astype(np.float32) / 255.0\n",
    "\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(train_features[100].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Label: {train_labels[100]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimized Data Pipeline (tf.data API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - Optimized for A100\n",
    "BATCH_SIZE = 64  # Increased from 8 for better GPU utilization\n",
    "N_SPLITS = 40\n",
    "EPOCHS = 2000\n",
    "LEARNING_RATE = 0.002\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"K-Fold Splits: {N_SPLITS}\")\n",
    "print(f\"Max Epochs: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def augment_image(image, label):\n",
    "    \"\"\"Data augmentation using tf.image for GPU acceleration\"\"\"\n",
    "    # Random shift (height and width by 1 pixel)\n",
    "    # Pad -> Random crop to simulate shift\n",
    "    image = tf.image.pad_to_bounding_box(image, 1, 1, 30, 30)\n",
    "    image = tf.image.random_crop(image, size=[28, 28, 1])\n",
    "    return image, label\n",
    "\n",
    "def create_train_dataset(x, y, batch_size, augment=True):\n",
    "    \"\"\"Create optimized training dataset with tf.data\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x), reshuffle_each_iteration=True)\n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_valid_dataset(x, y, batch_size):\n",
    "    \"\"\"Create optimized validation dataset\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_test_dataset(x, batch_size):\n",
    "    \"\"\"Create optimized test dataset\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "print(\"Data pipeline functions created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Build CNN model optimized for mixed precision training\"\"\"\n",
    "    model = Sequential([\n",
    "        # Input\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        \n",
    "        # Block 1\n",
    "        Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (5, 5), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (5, 5), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (5, 5), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3, 3)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((3, 3)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer with float32 for numerical stability in mixed precision\n",
    "        Dense(10, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and show model summary\n",
    "sample_model = build_model()\n",
    "sample_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training with K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Cross validation setup\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=42, shuffle=True)\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(patience=100, verbose=1, factor=0.5)\n",
    "early_stop = EarlyStopping(patience=160, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Results storage\n",
    "val_loss_min = []\n",
    "result = np.zeros((len(test_features), 10), dtype=np.float32)\n",
    "nth = 0\n",
    "\n",
    "# Create test dataset once (outside loop)\n",
    "test_dataset = create_test_dataset(test_features, BATCH_SIZE)\n",
    "\n",
    "print(f\"Starting {N_SPLITS}-Fold Cross Validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for train_index, valid_index in skf.split(train_features, train_labels):\n",
    "    nth += 1\n",
    "    print(f\"\\n[Fold {nth}/{N_SPLITS}]\")\n",
    "    \n",
    "    # Split data\n",
    "    x_train, x_valid = train_features[train_index], train_features[valid_index]\n",
    "    y_train, y_valid = train_labels[train_index], train_labels[valid_index]\n",
    "    \n",
    "    # Create optimized datasets\n",
    "    train_dataset = create_train_dataset(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "    valid_dataset = create_valid_dataset(x_valid, y_valid, BATCH_SIZE)\n",
    "    \n",
    "    # Build fresh model for each fold\n",
    "    model = build_model()\n",
    "    \n",
    "    # Compile with scaled loss for mixed precision\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE, epsilon=1e-07),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # ModelCheckpoint for this fold\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f'best_model_fold_{nth}.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load best weights and predict\n",
    "    model.load_weights(f'best_model_fold_{nth}.h5')\n",
    "    fold_pred = model.predict(test_dataset, verbose=0)\n",
    "    result += fold_pred / N_SPLITS\n",
    "    \n",
    "    # Save validation loss\n",
    "    val_loss_min.append(min(history.history['val_loss']))\n",
    "    print(f\"Fold {nth} completed. Best val_loss: {val_loss_min[-1]:.4f}\")\n",
    "    \n",
    "    # Clear session to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation loss statistics\n",
    "print(\"Validation Loss per Fold:\")\n",
    "for i, loss in enumerate(val_loss_min, 1):\n",
    "    print(f\"  Fold {i:2d}: {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nMean Validation Loss: {np.mean(val_loss_min):.4f}\")\n",
    "print(f\"Std Validation Loss: {np.std(val_loss_min):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "sub['digit'] = result.argmax(axis=1)\n",
    "\n",
    "# Display prediction distribution\n",
    "print(\"Prediction Distribution:\")\n",
    "print(sub['digit'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview submission\n",
    "print(\"Submission Preview:\")\n",
    "display(sub.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "output_filename = 'submission_optimized_A100.csv'\n",
    "sub.to_csv(output_filename, index=False)\n",
    "print(f\"Submission saved to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Performance Comparison Notes\n",
    "\n",
    "## Optimization Summary:\n",
    "\n",
    "| Optimization | Original | Optimized | Expected Speedup |\n",
    "|-------------|----------|-----------|------------------|\n",
    "| Precision | FP32 | Mixed FP16 | ~2-3x |\n",
    "| Batch Size | 8 | 64 | ~3-5x |\n",
    "| Data Pipeline | ImageDataGenerator | tf.data + prefetch | ~1.5-2x |\n",
    "| XLA Compilation | Disabled | Enabled | ~1.2-1.5x |\n",
    "\n",
    "**Total Expected Speedup: 5-10x faster on A100 GPU**\n",
    "\n",
    "## Notes:\n",
    "- Mixed precision training uses Tensor Cores on A100 for FP16 matrix operations\n",
    "- Output layer uses FP32 for numerical stability (softmax)\n",
    "- Larger batch size improves GPU utilization efficiency\n",
    "- tf.data prefetch allows overlapping data loading with GPU computation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
